# -*- coding: utf-8 -*-
"""Loan Default Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H61z1qTAwG0BQ8J2QwWXExNsxRV4vmEz
"""

# Step 1: Core Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Step 2: ML Imports
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# Step 3: Utility Imports
import warnings
warnings.filterwarnings('ignore')

"""### Loading `/content/testprevloans.csv`"""

df_testprevloans = pd.read_csv('/content/testprevloans.csv')
display(df_testprevloans.head())

"""### Loading `/content/trainperf.csv`"""

df_trainperf = pd.read_csv('/content/trainperf.csv')
display(df_trainperf.head())

"""### Loading `/content/trainprevloans.csv`"""

df_trainprevloans = pd.read_csv('/content/trainprevloans.csv')
display(df_trainprevloans.head())

"""### Loading `/content/testperf.csv`"""

df_testperf = pd.read_csv('/content/testperf.csv')
display(df_testperf.head())

"""### Loading `/content/SampleSubmission.csv`"""

df_sample_submission = pd.read_csv('/content/SampleSubmission.csv')
display(df_sample_submission.head())

"""### Loading `/content/traindemographics.csv`"""

df_traindemographics = pd.read_csv('/content/traindemographics.csv')
display(df_traindemographics.head())

"""### Loading `/content/testdemographics.csv`"""

df_testdemographics = pd.read_csv('/content/testdemographics.csv')
display(df_testdemographics.head())

def explore_dataset(df,name):
  print(f"Dataset: {name}")
  print(f"Shape: {df.shape}")
  print(f'Data types{df.dtypes}')
  print(f"\nMissing Values:")
  print(df.isnull().sum())

explore_dataset(df_traindemographics, "Train Demographics")
explore_dataset(df_trainperf, "Train Performance (TARGET)")
explore_dataset(df_trainprevloans, "Train Previous Loans")

# Convert date columns to datetime
def convert_dtypes(df, df_name):
    print(f"\nüîÑ CONVERTING DTYPES: {df_name}")

    date_columns = []
    for col in df.columns:
        if 'date' in col.lower():
            date_columns.append(col)
            df[col] = pd.to_datetime(df[col], errors='coerce')

    print(f"Converted date columns: {date_columns}")
    return df

# Convert dates in each dataset
df_traindemographics = convert_dtypes(df_traindemographics, "Demographics")
df_trainperf = convert_dtypes(df_trainperf, "Performance")
df_trainprevloans = convert_dtypes(df_trainprevloans, "Previous Loans")

print("\nüîç KEY INSIGHTS:")
print(f"‚Ä¢ Demographics: High missingness in bank_branch (99%), education (86%)")
print(f"‚Ä¢ Performance: Target 'good_bad_flag' is object (needs encoding)")
print(f"‚Ä¢ Previous Loans: 18K historical loans vs 4.3K current loans")
print(f"‚Ä¢ Referral data: Mostly missing across datasets")

def analyze_missingness(df, df_name):
    print(f"\nüìä MISSING VALUE ANALYSIS: {df_name}")
    print(f"{'='*50}")

    missing = df.isnull().sum()
    missing_pct = (missing / len(df)) * 100

    missing_df = pd.DataFrame({
        'missing_count': missing,
        'missing_percentage': missing_pct
    }).sort_values('missing_percentage', ascending=False)

    # Only show columns with missing values
    missing_df = missing_df[missing_df['missing_count'] > 0]

    if len(missing_df) > 0:
        display(missing_df)
    else:
        print("‚úÖ No missing values")

    return missing_df

# Analyze missingness for each dataset
missing_demo = analyze_missingness(df_traindemographics, "Demographics")
missing_perf = analyze_missingness(df_trainperf, "Performance")
missing_prev = analyze_missingness(df_trainprevloans, "Previous Loans")

df_traindemographics.employment_status_clients.unique()

print("üîç EMPLOYMENT STATUS DISTRIBUTION (Before Imputing):")
print("="*50)

employment_counts = df_traindemographics['employment_status_clients'].value_counts(dropna=False)
employment_pct = df_traindemographics['employment_status_clients'].value_counts(dropna=False, normalize=True) * 100

employment_df = pd.DataFrame({
    'count': employment_counts,
    'percentage': employment_pct
})

display(employment_df)

print(f"\nüìä Summary:")
print(f"‚Ä¢ Missing: {employment_df.loc[np.nan, 'count']} rows ({employment_df.loc[np.nan, 'percentage']:.1f}%)")
print(f"‚Ä¢ Present: {employment_counts.sum() - employment_counts[np.nan]} rows ({100 - employment_pct[np.nan]:.1f}%)")

# Impute with 'Unknown' to preserve data integrity
df_traindemographics['employment_status_clients'] = df_traindemographics['employment_status_clients'].fillna('Unknown')

print("‚úÖ Imputed 648 missing employment status values with 'Unknown'")
print("Updated distribution:")
print(df_traindemographics['employment_status_clients'].value_counts(normalize=True) * 100)

print("üîç PRE-MERGE EXPLORATION CHECKLIST")
print("="*50)

# 1. Check for duplicate customer IDs
print("\n1. CUSTOMER ID UNIQUENESS:")
print(f"Demographics: {df_traindemographics['customerid'].nunique()} unique / {len(df_traindemographics)} total")
print(f"Performance: {df_trainperf['customerid'].nunique()} unique / {len(df_trainperf)} total")
print(f"Previous Loans: {df_trainprevloans['customerid'].nunique()} unique / {len(df_trainprevloans)} total")

# 2. Check target variable distribution
print("\n2. TARGET VARIABLE DISTRIBUTION:")
print(df_trainperf['good_bad_flag'].value_counts(normalize=True))

# 3. Check date ranges for temporal validity
print("\n3. DATE RANGES:")
for df, name in [(df_trainperf, 'Performance'), (df_trainprevloans, 'Previous Loans')]:
    date_cols = [col for col in df.columns if 'date' in col.lower()]
    for col in date_cols:
        if df[col].dtype == 'datetime64[ns]':
            print(f"{name} - {col}: {df[col].min()} to {df[col].max()}")

# 4. Check for data type mismatches in merge keys
print("\n4. MERGE KEY COMPATIBILITY:")
print(f"customerid dtypes - Demographics: {df_traindemographics['customerid'].dtype}")
print(f"customerid dtypes - Performance: {df_trainperf['customerid'].dtype}")
print(f"customerid dtypes - Previous Loans: {df_trainprevloans['customerid'].dtype}")

df_trainprevloans.head()

print(" COLUMN EXPLORATION")
print("="*50)

# 1. Demographics deep dive
print("\n1. DEMOGRAPHICS COLUMNS:")
demo_cols = ['bank_account_type', 'bank_name_clients', 'employment_status_clients']
for col in demo_cols:
    print(f"\n{col}:")
    print(df_traindemographics[col].value_counts().head())

# 2. Performance data deep dive
print("\n2. PERFORMANCE COLUMNS (Current Loan):")
print("Loan amounts:", df_trainperf[['loanamount', 'totaldue', 'termdays']].describe())
print("\nLoan number distribution:")
print(df_trainperf['loannumber'].value_counts().sort_index())

# 3. Previous loans deep dive
print("\n3. PREVIOUS LOANS COLUMNS:")
print("Historical loan stats:", df_trainprevloans[['loanamount', 'totaldue', 'termdays']].describe())
print("\nLoan sequence per customer:")
loan_counts = df_trainprevloans.groupby('customerid')['loannumber'].max()
print(loan_counts.value_counts().sort_index())

# 4. Check if current loans exist in previous loans
print(f"\n4. DATA RELATIONSHIPS:")
current_loan_ids = set(df_trainperf['systemloanid'])
prev_loan_ids = set(df_trainprevloans['systemloanid'])
print(f"Current loans in previous data: {len(current_loan_ids & prev_loan_ids)}")
print(f"Unique customers with history: {df_trainprevloans['customerid'].nunique()}")

"""üéØ KEY INSIGHTS & MERGE STRATEGY
==================================================

üìä BUSINESS UNDERSTANDING:
* Customers take multiple loans over time (loannumber 2-27)
* Current loans are NEW - not in previous data (systemloanid mismatch)
* We're predicting performance of their NEXT loan
* Most customers have 1-3 previous loans

üîó MERGE STRATEGY:
1. LEFT JOIN: Performance ‚Üí Demographics (on customerid)
2. LEFT JOIN: Result ‚Üí Previous Loans (on customerid)
3. Keep ALL current loans, even if no demographic/history data

üìà FEATURE ENGINEERING OPPORTUNITIES:
* From Previous Loans: payment timing, loan frequency, amount trends
* From Demographics: bank preferences, employment stability
* Temporal features: days between loans, seasonality

‚ö†Ô∏è DATA QUALITY NOTES:
* 12 customers missing from Demographics (4346 vs 4368)
* 9 customers missing from Previous Loans (4359 vs 4368)
* Class imbalance: 78% Good vs 22% Bad loans
"""

print("1. Merging Performance + Demographics...")
df_merged = pd.merge(df_trainperf, df_traindemographics, on='customerid', how='left')
print(f"   After first merge: {df_merged.shape}")

print("2. Adding Previous Loans data...")
df_merged_with_prev = pd.merge(df_merged, df_trainprevloans, on='customerid', how='left', suffixes=('_current', '_prev'))
print(f"   After second merge: {df_merged_with_prev.shape}")

print(f"Final dataset shape: {df_merged_with_prev.shape}")
print(f"Rows with demographic data: {df_merged_with_prev['birthdate'].notna().sum()}")
print(f"Rows with previous loans: {df_merged_with_prev['systemloanid_prev'].notna().sum()}")

print("\nüìã MERGED DATASET COLUMNS:")
print(f"Performance columns: {[col for col in df_merged_with_prev.columns if '_current' in col or col in ['customerid', 'good_bad_flag']]}")

print(f"Demographics columns: {[col for col in df_merged_with_prev.columns if col in df_traindemographics.columns]}")

print(f"Previous loans columns: {[col for col in df_merged_with_prev.columns if '_prev' in col]}")

df_merged_with_prev.info()

print(f"We have {len(df_merged_with_prev)} rows but only {df_merged_with_prev['customerid'].nunique()} unique customers")

prev_loans_count = df_merged_with_prev.groupby('customerid')['systemloanid_prev'].count()
print(f"\nüìä Previous loans per customer:")
print(prev_loans_count.describe())

print(f"\n Customer with {prev_loans_count.max()} previous loans")

print("üîÑ CREATING AGGREGATED FEATURES (FIXED)")
print("="*50)

# 1. Create base dataset with correct column names
print("\n1. Creating base dataset...")
df_base = df_merged[['customerid', 'systemloanid', 'good_bad_flag'] +
                    list(df_traindemographics.columns)]
print(f"Base shape: {df_base.shape}")

# Check for duplicates
if df_base['systemloanid'].duplicated().any():
    print("‚ö†Ô∏è  Duplicate loans found - investigating...")
    duplicate_loans = df_base[df_base['systemloanid'].duplicated(keep=False)]
    print(f"Duplicate loans: {len(duplicate_loans)}")
else:
    print("‚úÖ No duplicate loans - safe to proceed")

# 2. Vectorized aggregation for basic stats
print("\n2. Aggregating basic loan history...")
prev_loan_agg = df_trainprevloans.groupby('customerid').agg(
    prev_loan_count=('systemloanid', 'count'),
    prev_loan_amount_mean=('loanamount', 'mean'),
    prev_loan_amount_max=('loanamount', 'max'),
    prev_loan_amount_min=('loanamount', 'min'),
    prev_loan_amount_std=('loanamount', 'std'),
    prev_total_due_mean=('totaldue', 'mean'),
    prev_term_days_mean=('termdays', 'mean'),
    prev_max_loannumber=('loannumber', 'max')
).round(2)

# Handle single-loan customers (std = 0)
prev_loan_agg['prev_loan_amount_std'] = prev_loan_agg['prev_loan_amount_std'].fillna(0)

# 3. Safe date operations
print("\n3. Calculating payment behavior...")
date_columns_available = all(col in df_trainprevloans.columns for col in ['closeddate', 'firstduedate', 'firstrepaiddate'])

if date_columns_available:
    # Vectorized date calculations
    df_prev_with_dates = df_trainprevloans.copy()

    # Days to close loan
    df_prev_with_dates['days_to_close'] = (df_prev_with_dates['closeddate'] - df_prev_with_dates['approveddate']).dt.days

    # Days to first repayment
    df_prev_with_dates['days_to_first_repay'] = (df_prev_with_dates['firstrepaiddate'] - df_prev_with_dates['firstduedate']).dt.days

    # Timely repayment flag
    df_prev_with_dates['timely_repayment'] = df_prev_with_dates['days_to_first_repay'] <= 0

    # Aggregate payment behavior
    payment_agg = df_prev_with_dates.groupby('customerid').agg(
        avg_days_to_close=('days_to_close', 'mean'),
        avg_days_to_first_repay=('days_to_first_repay', 'mean'),
        timely_repayment_ratio=('timely_repayment', 'mean'),
        total_timely_repayments=('timely_repayment', 'sum')
    ).round(2)
else:
    print("‚ö†Ô∏è  Date columns not available - skipping payment behavior features")
    payment_agg = pd.DataFrame(index=df_trainprevloans['customerid'].unique()).reset_index()

# 4. Add first-time borrower flag
print("\n4. Adding borrower flags...")
all_customers = set(df_base['customerid'])
customers_with_history = set(prev_loan_agg.index)

first_time_borrowers = all_customers - customers_with_history
print(f"First-time borrowers: {len(first_time_borrowers)}")

borrower_flags = pd.DataFrame({
    'customerid': list(all_customers),
    'is_first_time_borrower': [1 if cust in first_time_borrowers else 0 for cust in all_customers]
})

print(f"\n‚úÖ Ready for final merge:")
print(f"Base: {df_base.shape}")
print(f"Loan history: {prev_loan_agg.shape}")
print(f"Payment behavior: {payment_agg.shape if not payment_agg.empty else 'N/A'}")
print(f"Borrower flags: {borrower_flags.shape}")

print("üîç INVESTIGATING DUPLICATES")
print("="*50)

# Check the duplicate loans
duplicate_loans = df_base[df_base['systemloanid'].duplicated(keep=False)]
print("Duplicate loans found:")
display(duplicate_loans[['customerid', 'systemloanid', 'good_bad_flag']].head(10))

print(f"\nUnique duplicate systemloanids: {duplicate_loans['systemloanid'].nunique()}")
print(f"Customers with duplicates: {duplicate_loans['customerid'].nunique()}")

# Fix the borrower flags issue (only 1 row suggests an error)
print("\nüîß FIXING BORROWER FLAGS...")
print(f"All customers in base: {len(all_customers)}")
print(f"Customers with history: {len(customers_with_history)}")
print(f"First-time borrowers calculated: {len(first_time_borrowers)}")

# Recreate borrower flags correctly
borrower_flags = pd.DataFrame({
    'customerid': list(all_customers)
})
borrower_flags['is_first_time_borrower'] = borrower_flags['customerid'].isin(first_time_borrowers).astype(int)

print(f"Fixed borrower flags: {borrower_flags.shape}")
print(f"First-time borrowers: {borrower_flags['is_first_time_borrower'].sum()}")

print("\nüìä SUMMARY BEFORE MERGE:")
print(f"Will merge {len(df_base)} current loans with:")
print(f" - {len(prev_loan_agg)} customers with loan history")
print(f" - {len(payment_agg)} customers with payment behavior data")
print(f" - {borrower_flags['is_first_time_borrower'].sum()} first-time borrowers")

print("üîß FIXING COLUMN NAME CONFLICT")
print("="*50)

print("ISSUE: Duplicate 'customerid' column in df_base")
print("This happened during the merge - both datasets had 'customerid'")

# Check which customerid column has data
print(f"\nColumn value comparison:")
print(f"customerid_1 (first): {df_base.iloc[:, 0].name} - non-null: {df_base.iloc[:, 0].notna().sum()}")
print(f"customerid_2 (second): {df_base.iloc[:, 3].name} - non-null: {df_base.iloc[:, 3].notna().sum()}")

# Keep the first customerid column and drop the second
print(f"\nCLEANING DUPLICATE COLUMNS...")
df_base_clean = df_base.iloc[:, [0, 1, 2]]  # Keep first 3 columns: customerid, systemloanid, good_bad_flag
demographic_cols = df_base.columns[4:]  # Skip the duplicate customerid at index 3
df_base_clean = pd.concat([df_base_clean, df_base[demographic_cols]], axis=1)

print(f"Cleaned df_base shape: {df_base_clean.shape}")
print(f"Cleaned columns: {df_base_clean.columns.tolist()}")

# Now check for duplicates in the clean dataset
print(f"\nCHECKING FOR DUPLICATES IN CLEAN DATA...")
duplicate_loans = df_base_clean[df_base_clean.duplicated(subset=['systemloanid'], keep=False)]
print(f"Duplicate systemloanids found: {len(duplicate_loans)}")

if len(duplicate_loans) > 0:
    print("Removing duplicates...")
    df_base_final = df_base_clean.drop_duplicates(subset=['systemloanid'], keep='first')
    print(f"Final base shape: {df_base_final.shape}")
else:
    df_base_final = df_base_clean
    print("No duplicates found")

print(f"\n‚úÖ FINAL BASE DATASET READY:")
print(f"Shape: {df_base_final.shape}")
print(f"Columns: {df_base_final.columns.tolist()}")

print("üîÑ PERFORMING FINAL MERGE")
print("="*50)

# Recalculate borrower flags with clean data
all_customers_correct = set(df_base_final['customerid'].unique())
customers_with_history_correct = set(prev_loan_agg.index.unique())
first_time_borrowers_correct = all_customers_correct - customers_with_history_correct

print(f"Merge participants:")
print(f"- Base customers: {len(all_customers_correct)}")
print(f"- Customers with loan history: {len(customers_with_history_correct)}")
print(f"- First-time borrowers: {len(first_time_borrowers_correct)}")

# Create corrected borrower flags
borrower_flags_correct = pd.DataFrame({
    'customerid': list(all_customers_correct),
    'is_first_time_borrower': [1 if cust in first_time_borrowers_correct else 0 for cust in all_customers_correct]
})

print(f"\n1. Merging base + loan history...")
df_final = pd.merge(df_base_final, prev_loan_agg, left_on='customerid', right_index=True, how='left')
print(f"   After loan history: {df_final.shape}")

print(f"\n2. Merging + payment behavior...")
if not payment_agg.empty and 'customerid' in payment_agg.columns:
    df_final = pd.merge(df_final, payment_agg, on='customerid', how='left')
    print(f"   After payment behavior: {df_final.shape}")
else:
    print("   Skipping payment behavior (not available)")

print(f"\n3. Merging + borrower flags...")
df_final = pd.merge(df_final, borrower_flags_correct, on='customerid', how='left')
print(f"   After borrower flags: {df_final.shape}")

print(f"\n‚úÖ FINAL DATASET COMPLETE!")
print(f"Final shape: {df_final.shape}")
print(f"Target distribution:")
print(df_final['good_bad_flag'].value_counts(normalize=True))

print(f"\nüìä MISSING VALUES SUMMARY:")
missing_summary = (df_final.isnull().sum() / len(df_final) * 100).round(2)
missing_summary = missing_summary[missing_summary > 0]
if len(missing_summary) > 0:
    print("Columns with missing values:")
    print(missing_summary)
else:
    print("No missing values!")

print("üßπ FINAL DATA CLEANING")
print("="*50)

# 1. Drop high-missing columns
print(f"\n1. Dropping high-missing columns...")
cols_to_drop = ['bank_branch_clients', 'level_of_education_clients']
df_clean = df_final.drop(cols_to_drop, axis=1)
print(f"   After dropping: {df_clean.shape}")

# 2. Impute demographic missingness (25% - from customers without demographic data)
print(f"\n2. Imputing demographic missing values...")
demographic_cols = ['birthdate', 'bank_account_type', 'longitude_gps', 'latitude_gps',
                   'bank_name_clients', 'employment_status_clients']

# For categorical: impute with mode or 'Unknown'
df_clean['bank_account_type'] = df_clean['bank_account_type'].fillna('Unknown')
df_clean['bank_name_clients'] = df_clean['bank_name_clients'].fillna('Unknown')
df_clean['employment_status_clients'] = df_clean['employment_status_clients'].fillna('Unknown')

# For numeric: impute with median
df_clean['longitude_gps'] = df_clean['longitude_gps'].fillna(df_clean['longitude_gps'].median())
df_clean['latitude_gps'] = df_clean['latitude_gps'].fillna(df_clean['latitude_gps'].median())

# For birthdate: we'll handle in feature engineering
print(f"   Demographic imputation complete")

# 3. Handle missing previous loan data (first-time borrowers)
print(f"\n3. Handling first-time borrowers...")
prev_loan_cols = [col for col in df_clean.columns if col.startswith('prev_')]
for col in prev_loan_cols:
    df_clean[col] = df_clean[col].fillna(0)  # First-time borrowers have 0 history

print(f"   First-time borrowers handled: {df_clean['is_first_time_borrower'].sum()} customers")

print(f"\n‚úÖ FINAL CLEAN DATASET:")
print(f"Shape: {df_clean.shape}")
print(f"Missing values: {df_clean.isnull().sum().sum()} (should be only birthdate now)")
print(f"\nColumns: {df_clean.columns.tolist()}")

df_clean.isnull().sum()

df_clean.head()

df_clean.good_bad_flag.value_counts()

df_clean.employment_status_clients.unique()

df_clean.is_first_time_borrower.value_counts()

def explore_column(df, col_name):
    print(f"\nüìä {col_name}:")
    print(f"   Data type: {df[col_name].dtype}")
    print(f"   Non-null: {df[col_name].notna().sum()} / {len(df)} ({df[col_name].notna().sum()/len(df)*100:.1f}%)")

    if df[col_name].dtype in ['object', 'category']:
        # Categorical column
        value_counts = df[col_name].value_counts(dropna=False)
        print(f"   Unique values: {df[col_name].nunique()}")
        print(f"   Top 5 values:")
        for val, count in value_counts.head(5).items():
            print(f"     - {val}: {count} ({count/len(df)*100:.1f}%)")

    elif 'int' in str(df[col_name].dtype) or 'float' in str(df[col_name].dtype):
        # Numerical column
        print(f"   Min: {df[col_name].min():.2f}")
        print(f"   Max: {df[col_name].max():.2f}")
        print(f"   Mean: {df[col_name].mean():.2f}")
        print(f"   Median: {df[col_name].median():.2f}")
        print(f"   Std: {df[col_name].std():.2f}")

    elif 'datetime' in str(df[col_name].dtype):
        # Date column
        print(f"   Range: {df[col_name].min()} to {df[col_name].max()}")

    print("-" * 40)

# Explore each column systematically
columns_to_explore = [
    'customerid', 'systemloanid', 'good_bad_flag',  # Identifiers & target
    'birthdate', 'bank_account_type', 'longitude_gps', 'latitude_gps',  # Demographics
    'bank_name_clients', 'employment_status_clients',  # More demographics
    'prev_loan_count', 'prev_loan_amount_mean', 'prev_loan_amount_max',  # Loan history
    'prev_loan_amount_min', 'prev_loan_amount_std', 'prev_total_due_mean',  # More loan history
    'prev_term_days_mean', 'prev_max_loannumber', 'is_first_time_borrower'  # Final features
]

for col in columns_to_explore:
    if col in df_clean.columns:
        explore_column(df_clean, col)
    else:
        print(f"‚ùå {col} not found in dataframe")

print("\n1. EMPLOYMENT STATUS - 'Unknown' Analysis:")
employment_unknown = df_clean[df_clean['employment_status_clients'] == 'Unknown']
employment_known = df_clean[df_clean['employment_status_clients'] != 'Unknown']

print(f"   Customers with 'Unknown' employment: {len(employment_unknown)} ({len(employment_unknown)/len(df_clean)*100:.1f}%)")
print(f"   Default rate - Unknown: {employment_unknown['good_bad_flag'].value_counts(normalize=True)['Bad']:.1%}")
print(f"   Default rate - Known: {employment_known['good_bad_flag'].value_counts(normalize=True)['Bad']:.1%}")

print("\n2. BANK NAME - 'Unknown' Analysis:")
bank_unknown = df_clean[df_clean['bank_name_clients'] == 'Unknown']
bank_known = df_clean[df_clean['bank_name_clients'] != 'Unknown']

print(f"   Customers with 'Unknown' bank: {len(bank_unknown)} ({len(bank_unknown)/len(df_clean)*100:.1f}%)")
print(f"   Default rate - Unknown bank: {bank_unknown['good_bad_flag'].value_counts(normalize=True)['Bad']:.1%}")
print(f"   Default rate - Known bank: {bank_known['good_bad_flag'].value_counts(normalize=True)['Bad']:.1%}")

print("\n3. BIRTHDATE - Missing Analysis:")
birthdate_missing = df_clean[df_clean['birthdate'].isna()]
birthdate_present = df_clean[df_clean['birthdate'].notna()]

print(f"   Customers missing birthdate: {len(birthdate_missing)} ({len(birthdate_missing)/len(df_clean)*100:.1f}%)")
print(f"   Default rate - Missing birthdate: {birthdate_missing['good_bad_flag'].value_counts(normalize=True)['Bad']:.1%}")
print(f"   Default rate - With birthdate: {birthdate_present['good_bad_flag'].value_counts(normalize=True)['Bad']:.1%}")

print("\n4. CLASS IMBALANCE Analysis:")
print(f"   Good loans: {len(df_clean[df_clean['good_bad_flag'] == 'Good'])} ({78.2}%)")
print(f"   Bad loans: {len(df_clean[df_clean['good_bad_flag'] == 'Bad'])} ({21.8}%)")
print(f"   Imbalance ratio: {3416/952:.1f}:1")

"""Strategy:
Preserve "Unknown" categories - they carry predictive signal

Create "missing" flags for birthdate

Use balanced class weights in modeling

No need for sophisticated imputation - patterns are clear
"""

# -------------------------------
# 0. Ensure age column exists
current_year = pd.Timestamp.now().year
df_final['age'] = current_year - pd.to_datetime(df_final['birthdate']).dt.year

# Fill missing ages with median
age_median = df_final['age'].median()
df_final['age'] = df_final['age'].fillna(age_median)
df_final['target'] = (df_final['good_bad_flag'] == 'Bad').astype(int)

# -------------------------------
# 1. Explore Age Distribution
print("üîç AGE DISTRIBUTION EXPLORATION")
print("="*40)

age_min = df_final['age'].min()
age_max = df_final['age'].max()
age_summary = df_final['age'].describe()

print(f"Minimum age: {age_min}")
print(f"Maximum age: {age_max}")
print("Age summary statistics:")
print(age_summary)

# Optionally, check count of customers under 18
under_18_count = (df_final['age'] < 18).sum()
print(f"Number of customers under 18: {under_18_count}")

# -------------------------------
# 2. Explore Loan Count Distribution
print("\nüîç LOAN COUNT DISTRIBUTION EXPLORATION")
print("="*40)

loan_count_min = df_final['prev_loan_count'].min()
loan_count_max = df_final['prev_loan_count'].max()
loan_count_summary = df_final['prev_loan_count'].describe()

print(f"Minimum loan count: {loan_count_min}")
print(f"Maximum loan count: {loan_count_max}")
print("Loan count summary statistics:")
print(loan_count_summary)

# Check number of customers in each proposed bin
loan_bins = [-1, 0, 3, 10, 100]
loan_bin_labels = ['None', 'Low', 'Medium', 'High']
loan_count_bins = pd.cut(df_final['prev_loan_count'], bins=loan_bins, labels=loan_bin_labels)
print("Loan count bins distribution:")
print(loan_count_bins.value_counts())

# -------------------------------
# 3. Explore Target Balance
print("\nüîç TARGET BALANCE EXPLORATION")
print("="*40)

target_counts = df_final['target'].value_counts()
target_percent = df_final['target'].value_counts(normalize=True)
print("Target counts:")
print(target_counts)
print("Target percentages:")
print(target_percent)

df_final = df_clean.copy()

# -------------------------------
# 1. Preserve "Unknown" categories

# -------------------------------
# 2. Birthdate and Age
df_final['birthdate_missing'] = df_final['birthdate'].isna().astype(int)

current_year = pd.Timestamp.now().year
df_final['age'] = current_year - pd.to_datetime(df_final['birthdate']).dt.year
df_final['age'] = df_final['age'].fillna(df_final['age'].median())

# Corrected age bins
df_final['age_group'] = pd.cut(
    df_final['age'],
    bins=[29, 35, 45, 55, 100],
    labels=['29-35', '36-45', '46-55', '55+']
)

# -------------------------------
# 3. Loan behavior (corrected bins)
df_final['has_loan_history'] = (df_final['prev_loan_count'] > 0).astype(int)

loan_bins = [0, 3, 10, 26]
loan_labels = ['Low', 'Medium', 'High']
df_final['loan_count_category'] = pd.cut(
    df_final['prev_loan_count'],
    bins=loan_bins,
    labels=loan_labels,
    include_lowest=True
)

# -------------------------------
# 4. Target encoding
df_final['target'] = (df_final['good_bad_flag'] == 'Bad').astype(int)

df_final['age_group'] = df_final['age_group'].cat.add_categories('Unknown').fillna('Unknown')

df_final.isnull().sum()

print("üöÄ PREPARING FOR MODEL TRAINING")
print("="*50)

# Select features for modeling (exclude identifiers and raw dates)
feature_columns = [
    # Demographic features
    'bank_account_type', 'longitude_gps', 'latitude_gps',
    'bank_name_clients', 'employment_status_clients',
    'birthdate_missing', 'age', 'age_group',

    # Loan history features
    'prev_loan_count', 'prev_loan_amount_mean', 'prev_loan_amount_max',
    'prev_loan_amount_min', 'prev_loan_amount_std', 'prev_total_due_mean',
    'prev_term_days_mean', 'prev_max_loannumber', 'is_first_time_borrower',
    'has_loan_history', 'loan_count_category'
]

target_column = 'target'

print(f"üìã SELECTED FEATURES: {len(feature_columns)}")
print(f"üéØ TARGET: {target_column}")

# Create feature matrix and target vector
X = df_final[feature_columns]
y = df_final[target_column]

print(f"\nüìä DATASET READY FOR MODELING:")
print(f"   Features (X): {X.shape}")
print(f"   Target (y): {y.shape}")
print(f"   Class distribution: {y.value_counts(normalize=True).to_dict()}")

# Check data types
print(f"\nüîç FEATURE TYPES:")
print(f"   Numerical features: {len(X.select_dtypes(include=['int64', 'float64']).columns)}")
print(f"   Categorical features: {len(X.select_dtypes(include=['object', 'category']).columns)}")

print(f"\n‚úÖ READY FOR NEXT STEP:")
print("   We can now:")
print("   1. Encode categorical variables")
print("   2. Split into train/test sets")
print("   3. Train our first model")

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline



numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

# -------------------------------
# 2. Split into train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# -------------------------------

# 3. Preprocessing for categorical variables
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)
    ],
    remainder='passthrough'  # keep numerical features as-is
)

# -------------------------------
# 4. Build pipeline with Random Forest (balanced)
clf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))
])

# -------------------------------
# 5. Train the model
clf.fit(X_train, y_train)

# -------------------------------
# 6. Quick evaluation
y_pred = clf.predict(X_test)
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

print("‚öñÔ∏è  ADDRESSING CLASS IMBALANCE")
print("="*50)

print("CURRENT PERFORMANCE ISSUES:")
print("‚Ä¢ Good class (0): Decent performance (precision: 0.80, recall: 0.89)")
print("‚Ä¢ Bad class (1): Poor performance (precision: 0.31, recall: 0.17)")
print("‚Ä¢ Model is biased toward majority class")

print("\nüõ†Ô∏è  STRATEGIES TO IMPROVE:")
print("1. Use class weights in Random Forest")
print("2. Try different algorithms (XGBoost, LightGBM)")
print("4. Feature importance analysis")

print("\nüîß IMPLEMENTING CLASS WEIGHTS...")

# Identify categorical columns that still contain strings
categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()
print(f"Categorical columns needing encoding: {categorical_columns}")

# Apply LabelEncoder to ALL categorical columns
from sklearn.preprocessing import LabelEncoder

X_encoded = X.copy()
label_encoders = {}

for col in categorical_columns:
    le = LabelEncoder()
    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))
    label_encoders[col] = le
    print(f"Encoded {col}: {len(le.classes_)} categories")

print(f"\n‚úÖ ENCODING COMPLETE")
print(f"All features are now numerical")

# Recreate train/test split with encoded data
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(f"\nüìä DATA READY:")
print(f"X_train: {X_train.shape}, X_test: {X_test.shape}")

print("\nüå≤ TRAINING BALANCED RANDOM FOREST...")
rf_balanced = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',
    random_state=42,
    max_depth=10
)

rf_balanced.fit(X_train, y_train)
y_pred_balanced = rf_balanced.predict(X_test)

print("\n‚úÖ BALANCED RANDOM FOREST RESULTS:")
print(classification_report(y_test, y_pred_balanced))

"""üìä RESULTS ANALYSIS & NEXT STEPS
‚úÖ IMPROVEMENT ACHIEVED

Bad loan RECALL improved: 0.17 ‚Üí 0.43 (+153% improvement!)

Now detecting 43% of actual bad loans vs only 17% before

Trade-off: Precision decreased (0.31 ‚Üí 0.29)

‚ö†Ô∏è CURRENT CHALLENGES

Still low precision (29%): Many false alarms

Accuracy dropped: 74% ‚Üí 64% (expected with class balancing)

F1-score for bad loans: 0.35 (needs improvement)




"""

print("\nüîç FEATURE IMPORTANCE ANALYSIS:")
importances = rf_balanced.feature_importances_
feature_importance_df = pd.DataFrame({
    'feature': X_encoded.columns,
    'importance': importances
}).sort_values('importance', ascending=False)

print("Top 10 Most Important Features:")
print(feature_importance_df.head(10).to_string(index=False))

from sklearn.metrics import classification_report, precision_recall_fscore_support
print("üöÄ TRAINING XGBOOST FOR IMBALANCED DATA")
print("="*50)

try:
    from xgboost import XGBClassifier

    print("XGBoost Advantages:")
    print("‚Ä¢ Built-in handling of imbalanced data")
    print("‚Ä¢ Better performance on tabular data")
    print("‚Ä¢ Automatic feature importance")

    # Train XGBoost with scale_pos_weight for imbalance
    scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])
    print(f"\nScale positive weight: {scale_pos_weight:.2f}")

    xgb_model = XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        scale_pos_weight=scale_pos_weight,  # Handles class imbalance
        random_state=42,
        eval_metric='logloss'
    )

    xgb_model.fit(X_train, y_train)
    y_pred_xgb = xgb_model.predict(X_test)

    print("\n‚úÖ XGBOOST RESULTS:")
    print(classification_report(y_test, y_pred_xgb))

    # XGBoost feature importance
    print("\nüîç XGBOOST FEATURE IMPORTANCE:")
    xgb_importances = xgb_model.feature_importances_
    xgb_importance_df = pd.DataFrame({
        'feature': X_encoded.columns,
        'importance': xgb_importances
    }).sort_values('importance', ascending=False)

    print("Top 10 Most Important Features (XGBoost):")
    print(xgb_importance_df.head(10).to_string(index=False))

    print(f"\nüìä COMPARISON:")
    print("Random Forest vs XGBoost for Bad Loan Detection:")
    rf_precision, rf_recall, rf_f1, _ = precision_recall_fscore_support(y_test, y_pred_balanced, average='binary')
    xgb_precision, xgb_recall, xgb_f1, _ = precision_recall_fscore_support(y_test, y_pred_xgb, average='binary')

    print(f"               RF      XGBoost")
    print(f"Precision:   {rf_precision:.3f}     {xgb_precision:.3f}")
    print(f"Recall:      {rf_recall:.3f}     {xgb_recall:.3f}")
    print(f"F1-Score:    {rf_f1:.3f}     {xgb_f1:.3f}")

except ImportError:
    print("‚ùå XGBoost not available. Installing...")
    !pip install xgboost
    from xgboost import XGBClassifier
    print("‚úÖ XGBoost installed successfully")

print("üéõÔ∏è  HYPERPARAMETER TUNING FOR BETTER PRECISION")
print("="*50)

print("GOAL: Improve precision while maintaining good recall")
print("Current: Precision=0.30, Recall=0.48")
print("Target: Precision > 0.40, Recall > 0.40")

from sklearn.model_selection import GridSearchCV

print("\nüîß TUNING XGBOOST PARAMETERS...")

# Define parameter grid
param_grid = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.05, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0],
    'scale_pos_weight': [3.0, 3.5, 4.0]  # Fine-tune class weights
}

xgb = XGBClassifier(
    n_estimators=100,
    random_state=42,
    eval_metric='logloss'
)

# Use F1-score as scoring metric (balances precision and recall)
grid_search = GridSearchCV(
    xgb, param_grid,
    scoring='f1',  # Optimize for F1-score
    cv=3,
    n_jobs=-1,
    verbose=1
)

print("Starting grid search (this may take a few minutes)...")
grid_search.fit(X_train, y_train)

print(f"\n‚úÖ BEST PARAMETERS FOUND:")
print(grid_search.best_params_)

# Train with best parameters
best_xgb = grid_search.best_estimator_
y_pred_tuned = best_xgb.predict(X_test)

print("\nüéØ TUNED XGBOOST RESULTS:")
print(classification_report(y_test, y_pred_tuned))

# Compare before/after tuning
tuned_precision, tuned_recall, tuned_f1, _ = precision_recall_fscore_support(y_test, y_pred_tuned, average='binary')

print(f"\nüìä TUNING IMPROVEMENT:")
print(f"               Before     After")
print(f"Precision:   {xgb_precision:.3f}      {tuned_precision:.3f}")
print(f"Recall:      {xgb_recall:.3f}      {tuned_recall:.3f}")
print(f"F1-Score:    {xgb_f1:.3f}      {tuned_f1:.3f}")

"""STRATEGIC ANALYSIS OF TUNING RESULTS
üéØ TRADE-OFF ANALYSIS

RECALL IMPROVED: 0.48 ‚Üí 0.64 (+33% improvement!)

PRECISION DECREASED: 0.30 ‚Üí 0.27

F1-Score improved: 0.37 ‚Üí 0.38

‚ö†Ô∏è CURRENT SITUATION
----------------------------------------------

Detecting 64% of bad loans (excellent recall)

Precision only 27%: 73% of 'bad' predictions are false alarms

Business impact: Catch more bad loans but higher false positive rate

###üí° STRATEGIC OPTIONS
OPTION 1: High Recall Strategy (Current)
----------------------------------------------

Recall: 64%, Precision: 27%

Best for: Risk-averse lenders, regulatory compliance

Use case: Flag more loans for manual review


OPTION 2: Balanced Strategy
----------------------------------------------

Target: Recall ~50%, Precision ~40%

How: Adjust prediction threshold

Best for: Balanced risk management

OPTION 3: High Precision Strategy
----------------------------------------------

Target: Precision > 50%, Recall ~30%

How: Use threshold moving

Best for: Automated rejection systems
"""

print(f"\nüîß QUICK THRESHOLD ADJUSTMENT:")
# Get prediction probabilities instead of hard classes
y_pred_proba = best_xgb.predict_proba(X_test)[:, 1]

# Try different thresholds
thresholds = [0.3, 0.4, 0.5, 0.6]
print(f"\nTesting different prediction thresholds:")
for threshold in thresholds:
    y_pred_adjusted = (y_pred_proba >= threshold).astype(int)
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_adjusted, average='binary')
    print(f"Threshold {threshold}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}")

print("üéØ IMPLEMENTING HIGH PRECISION STRATEGY")
print("="*50)

print("GOAL: Precision > 50% while maintaining reasonable recall")
print("Current best: Threshold 0.6 ‚Üí Precision=0.33, Recall=0.22")

# Find optimal threshold for high precision
print("\nüîç FINDING OPTIMAL HIGH-PRECISION THRESHOLD...")
y_pred_proba = best_xgb.predict_proba(X_test)[:, 1]

# Test higher thresholds for better precision
high_precision_thresholds = [0.65, 0.7, 0.75, 0.8, 0.85]
print(f"\nTesting high-precision thresholds:")
best_f1 = 0
best_threshold = 0.5

for threshold in high_precision_thresholds:
    y_pred_adjusted = (y_pred_proba >= threshold).astype(int)
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_adjusted, average='binary')
    print(f"Threshold {threshold}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}")

    if precision >= 0.4 and f1 > best_f1:
        best_f1 = f1
        best_threshold = threshold

print(f"\n‚úÖ RECOMMENDED HIGH-PRECISION THRESHOLD: {best_threshold}")

# Apply the best high-precision threshold
y_pred_high_precision = (y_pred_proba >= best_threshold).astype(int)
final_precision, final_recall, final_f1, _ = precision_recall_fscore_support(y_test, y_pred_high_precision, average='binary')

print(f"\nüéØ FINAL HIGH-PRECISION PERFORMANCE:")
print(f"Precision: {final_precision:.3f} ({final_precision*100:.1f}% of predicted bad loans are actually bad)")
print(f"Recall:    {final_recall:.3f} ({final_recall*100:.1f}% of actual bad loans are detected)")
print(f"F1-Score:  {final_f1:.3f}")

print(f"\nüìä BUSINESS IMPACT:")
print(f"‚Ä¢ When model says 'BAD LOAN', it's correct {final_precision*100:.0f}% of the time")
print(f"‚Ä¢ Catches {final_recall*100:.0f}% of actual bad loans")
print(f"‚Ä¢ Good for: Automated rejection systems, low false positive rate")

print(f"\nüî¢ PREDICTION DISTRIBUTION:")
print(f"Total loans: {len(y_pred_high_precision)}")
print(f"Predicted Good: {(y_pred_high_precision == 0).sum()}")
print(f"Predicted Bad:  {(y_pred_high_precision == 1).sum()}")
print(f"Actual Bad loans: {y_test.sum()}")
print(f"Bad loans caught: {(y_pred_high_precision & y_test).sum()}")

print("‚öñÔ∏è IMPLEMENTING BALANCED STRATEGY")
print("="*50)

print("GOAL: Balance precision and recall ~40-50% each")
print("Current: Precision=0.27, Recall=0.64 (too low precision)")

# Find optimal balanced threshold
print("\nüîç FINDING OPTIMAL BALANCED THRESHOLD...")
y_pred_proba = best_xgb.predict_proba(X_test)[:, 1]

# Test thresholds for balanced performance
balanced_thresholds = [0.35, 0.4, 0.45, 0.5, 0.55]
print(f"\nTesting balanced thresholds:")
best_balanced_score = 0
best_balanced_threshold = 0.5

for threshold in balanced_thresholds:
    y_pred_adjusted = (y_pred_proba >= threshold).astype(int)
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_adjusted, average='binary')

    # Balanced score: harmonic mean of precision and recall
    balanced_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    print(f"Threshold {threshold}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}, Balanced Score={balanced_score:.3f}")

    if balanced_score > best_balanced_score:
        best_balanced_score = balanced_score
        best_balanced_threshold = threshold

print(f"\n‚úÖ OPTIMAL BALANCED THRESHOLD: {best_balanced_threshold}")

# Apply the best balanced threshold
y_pred_balanced_strategy = (y_pred_proba >= best_balanced_threshold).astype(int)
bal_precision, bal_recall, bal_f1, _ = precision_recall_fscore_support(y_test, y_pred_balanced_strategy, average='binary')

print(f"\nüéØ FINAL BALANCED PERFORMANCE:")
print(f"Precision: {bal_precision:.3f} ({bal_precision*100:.1f}% of predicted bad loans are actually bad)")
print(f"Recall:    {bal_recall:.3f} ({bal_recall*100:.1f}% of actual bad loans are detected)")
print(f"F1-Score:  {bal_f1:.3f}")

print(f"\nüìä BUSINESS IMPACT - BALANCED STRATEGY:")
print(f"‚Ä¢ When model flags 'BAD LOAN', it's correct {bal_precision*100:.0f}% of the time")
print(f"‚Ä¢ Catches {bal_recall*100:.0f}% of actual bad loans")
print(f"‚Ä¢ Balanced approach: Good mix of accuracy and coverage")

print(f"\nüî¢ PREDICTION DISTRIBUTION:")
print(f"Total loans: {len(y_pred_balanced_strategy)}")
print(f"Predicted Good: {(y_pred_balanced_strategy == 0).sum()}")
print(f"Predicted Bad:  {(y_pred_balanced_strategy == 1).sum()}")
print(f"Actual Bad loans: {y_test.sum()}")
print(f"Bad loans caught: {(y_pred_balanced_strategy & y_test).sum()}")
print(f"False positives: {(y_pred_balanced_strategy == 1) & (y_test == 0).sum()}")

print(f"\nüí° RECOMMENDATION:")
print(f"Balanced strategy provides better business value than extreme high-precision")

import shap
import matplotlib.pyplot as plt

print("üî¨ STARTING SHAP ANALYSIS")
print("="*50)

# Create SHAP explainer for XGBoost
explainer = shap.TreeExplainer(best_xgb)
shap_values = explainer.shap_values(X_test)

# 1Ô∏è‚É£ Global feature importance
print("\nüåç GLOBAL FEATURE IMPORTANCE")
shap.summary_plot(shap_values, X_test, plot_type="bar", max_display=10)

# 2Ô∏è‚É£ Detailed summary (beeswarm)
print("\nüêù DETAILED FEATURE EFFECTS")
shap.summary_plot(shap_values, X_test, max_display=10)

# 3Ô∏è‚É£ Local explanation for first test sample
print("\nüîé LOCAL EXPLANATION (first sample)")
shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])

import pandas as pd

# SHAP values
shap_values_array = shap_values  # already computed from explainer

# Feature names
feature_names = X_test.columns

# Create a DataFrame with SHAP values
shap_df = pd.DataFrame(shap_values_array, columns=feature_names)

# Optionally, compute mean absolute SHAP for each feature (global importance)
shap_mean_abs = shap_df.abs().mean().sort_values(ascending=False)

# Print top 10 features with their mean absolute SHAP values
print("üåü Top 10 Features by Mean Absolute SHAP Value:")
print(shap_mean_abs.head(10).to_string())

"""# Test Data

"""

print("üß™ PROCESSING TEST DATA")
print("="*50)

# 1. Load test datasets
print("1. üì• LOADING TEST DATA...")
df_test_demo = pd.read_csv('testdemographics.csv')
df_test_perf = pd.read_csv('testperf.csv')
df_test_prev = pd.read_csv('testprevloans.csv')

print(f"   Test Demographics: {df_test_demo.shape}")
print(f"   Test Performance: {df_test_perf.shape}")
print(f"   Test Previous Loans: {df_test_prev.shape}")

# 2. Apply same cleaning as training data
print("\n2. üßπ CLEANING TEST DATA...")

# Drop same columns as training
df_test_demo_clean = df_test_demo.drop(['bank_branch_clients', 'level_of_education_clients'], axis=1, errors='ignore')
df_test_perf_clean = df_test_perf.drop(['referredby'], axis=1, errors='ignore')
df_test_prev_clean = df_test_prev.drop(['referredby'], axis=1, errors='ignore')

# Impute employment status with 'Unknown'
df_test_demo_clean['employment_status_clients'] = df_test_demo_clean['employment_status_clients'].fillna('Unknown')

print(f"   Cleaning complete")

# 3. Merge test datasets
print("\n3. üîó MERGING TEST DATA...")

# First merge: Performance + Demographics
df_test_merged = pd.merge(df_test_perf_clean, df_test_demo_clean, on='customerid', how='left')
print(f"   After demo merge: {df_test_merged.shape}")

# 4. Feature engineering for test data
print("\n4. üîß FEATURE ENGINEERING ON TEST DATA...")

# Age features
df_test_merged['birthdate_missing'] = df_test_merged['birthdate'].isna().astype(int)
current_year = pd.Timestamp.now().year
df_test_merged['age'] = current_year - pd.to_datetime(df_test_merged['birthdate']).dt.year
age_median = df_test_merged['age'].median()
df_test_merged['age'] = df_test_merged['age'].fillna(age_median)
df_test_merged['age_group'] = pd.cut(df_test_merged['age'], bins=[0, 25, 35, 45, 55, 100],
                                    labels=['18-25', '26-35', '36-45', '46-55', '55+'])

print(f"   Basic features created")

print("\n5. üìä AGGREGATING PREVIOUS LOANS FOR TEST DATA...")

# Aggregate previous loans (same as training)
prev_loan_agg_test = df_test_prev_clean.groupby('customerid').agg({
    'systemloanid': 'count',
    'loanamount': ['mean', 'max', 'min', 'std'],
    'totaldue': ['mean'],
    'termdays': ['mean'],
    'loannumber': 'max'
}).round(2)

# Flatten column names
prev_loan_agg_test.columns = ['prev_' + '_'.join(col).strip() for col in prev_loan_agg_test.columns]
prev_loan_agg_test = prev_loan_agg_test.reset_index()

# Handle single-loan customers (std = 0)
if 'prev_loanamount_std' in prev_loan_agg_test.columns:
    prev_loan_agg_test['prev_loanamount_std'] = prev_loan_agg_test['prev_loanamount_std'].fillna(0)

print(f"   Previous loans aggregated: {prev_loan_agg_test.shape}")

# 6. Final test dataset merge
print("\n6. üîó FINAL TEST DATASET MERGE...")
df_test_final = pd.merge(df_test_merged, prev_loan_agg_test, on='customerid', how='left')

# Add first-time borrower flag
all_test_customers = set(df_test_merged['customerid'].unique())
customers_with_test_history = set(prev_loan_agg_test['customerid'].unique())
first_time_test_borrowers = all_test_customers - customers_with_test_history

borrower_flags_test = pd.DataFrame({
    'customerid': list(all_test_customers),
    'is_first_time_borrower': [1 if cust in first_time_test_borrowers else 0 for cust in all_test_customers]
})

df_test_final = pd.merge(df_test_final, borrower_flags_test, on='customerid', how='left')

# Fill missing previous loan data with 0
prev_loan_cols = [col for col in df_test_final.columns if col.startswith('prev_')]
for col in prev_loan_cols:
    df_test_final[col] = df_test_final[col].fillna(0)

print(f"   Final test dataset: {df_test_final.shape}")

print("üîß FIXING TEST DATA COLUMN NAMES")
print("="*50)

print("ISSUE: Column names mismatch between training and test data")
print("Need to check actual column names in test data")

# Check available columns in test data
print("\nüìã AVAILABLE COLUMNS IN TEST DATA:")
print("Test Demographics:", df_test_demo_clean.columns.tolist())
print("Test Performance:", df_test_perf_clean.columns.tolist())
print("Test Previous Loans:", df_test_prev_clean.columns.tolist())
print("Test Merged:", df_test_final.columns.tolist())

print("\nüîç CHECKING PREVIOUS LOAN AGGREGATION...")
print("Previous loan aggregation columns:", prev_loan_agg_test.columns.tolist() if 'prev_loan_agg_test' in locals() else "Not created")

# Recreate the aggregation with correct column names
print("\nüîÑ RECREATING AGGREGATION WITH CORRECT COLUMNS...")

# Check if test previous loans has the same structure
print("Test previous loans columns:", df_test_prev_clean.columns.tolist())

# Re-aggregate with correct column mapping
prev_loan_agg_test_fixed = df_test_prev_clean.groupby('customerid').agg({
    'systemloanid': 'count',
    'loanamount': ['mean', 'max', 'min', 'std'],
    'totaldue': ['mean'],
    'termdays': ['mean'],
    'loannumber': 'max'
}).round(2)

# Flatten column names correctly
prev_loan_agg_test_fixed.columns = ['prev_' + '_'.join(col).strip() for col in prev_loan_agg_test_fixed.columns]
prev_loan_agg_test_fixed = prev_loan_agg_test_fixed.reset_index()

print("Fixed aggregation columns:", prev_loan_agg_test_fixed.columns.tolist())

# Re-merge with fixed aggregation
print("\nüîó RE-MERGING WITH FIXED AGGREGATION...")
df_test_final_fixed = pd.merge(df_test_merged, prev_loan_agg_test_fixed, on='customerid', how='left')

# Add first-time borrower flag
all_test_customers = set(df_test_merged['customerid'].unique())
customers_with_test_history = set(prev_loan_agg_test_fixed['customerid'].unique())
first_time_test_borrowers = all_test_customers - customers_with_test_history

borrower_flags_test = pd.DataFrame({
    'customerid': list(all_test_customers),
    'is_first_time_borrower': [1 if cust in first_time_test_borrowers else 0 for cust in all_test_customers]
})

df_test_final_fixed = pd.merge(df_test_final_fixed, borrower_flags_test, on='customerid', how='left')

# Fill missing previous loan data with 0
prev_loan_cols_fixed = [col for col in df_test_final_fixed.columns if col.startswith('prev_')]
for col in prev_loan_cols_fixed:
    df_test_final_fixed[col] = df_test_final_fixed[col].fillna(0)

print(f"Fixed test dataset: {df_test_final_fixed.shape}")
print("Fixed columns:", df_test_final_fixed.columns.tolist())

print("üîß COMPLETE COLUMN NAME FIX")
print("="*50)

print("ISSUE: Multiple column name mismatches")
print("Need to map all aggregated column names")

# Check what columns we actually have
print("\nüìã ACTUAL COLUMNS IN TEST DATA:")
actual_columns = df_test_final_fixed.columns.tolist()
print(actual_columns)

print("\nüîÑ COMPLETE COLUMN MAPPING...")

# Complete mapping of all aggregated columns
full_column_mapping = {
    'prev_systemloanid_count': 'prev_loan_count',
    'prev_loannumber_max': 'prev_max_loannumber',
    'prev_loanamount_mean': 'prev_loan_amount_mean',
    'prev_loanamount_max': 'prev_loan_amount_max',
    'prev_loanamount_min': 'prev_loan_amount_min',
    'prev_loanamount_std': 'prev_loan_amount_std',
    'prev_totaldue_mean': 'prev_total_due_mean',
    'prev_termdays_mean': 'prev_term_days_mean'
}

# Apply all renames
df_test_final_fixed = df_test_final_fixed.rename(columns=full_column_mapping)
print(f"All columns renamed to match training data")

# Create missing features
print("\nüîß CREATING FINAL FEATURES...")
df_test_final_fixed['has_loan_history'] = (df_test_final_fixed['prev_loan_count'] > 0).astype(int)
df_test_final_fixed['loan_count_category'] = pd.cut(
    df_test_final_fixed['prev_loan_count'],
    bins=[-1, 0, 3, 10, 100],
    labels=['None', 'Low', 'Medium', 'High']
)

print(f"Final dataset shape: {df_test_final_fixed.shape}")
print(f"Final columns: {df_test_final_fixed.columns.tolist()}")

# Verify all required features exist
print("\n‚úÖ VERIFYING REQUIRED FEATURES...")
missing_features = [col for col in test_feature_columns if col not in df_test_final_fixed.columns]
if missing_features:
    print(f"‚ùå Missing features: {missing_features}")
else:
    print(f"‚úÖ All {len(test_feature_columns)} required features available")

# Create final test feature matrix
X_test_final = df_test_final_fixed[test_feature_columns].copy()
print(f"Final test feature matrix: {X_test_final.shape}")

# Apply encoding
print("\nüî¢ APPLYING FINAL ENCODING...")
categorical_columns_test = X_test_final.select_dtypes(include=['object', 'category']).columns.tolist()

for col in categorical_columns_test:
    if col in label_encoders:
        X_test_final[col] = X_test_final[col].astype(str)
        unseen_mask = ~X_test_final[col].isin(label_encoders[col].classes_)
        if unseen_mask.any():
            print(f"   Mapping {unseen_mask.sum()} unseen categories in {col}")
            X_test_final.loc[unseen_mask, col] = label_encoders[col].classes_[0]
        X_test_final[col] = label_encoders[col].transform(X_test_final[col])
        print(f"   Encoded {col}")
    else:
        print(f"   No encoder for {col}, using raw values")

print("Encoding complete!")

# Generate final predictions
print("\nüéØ GENERATING FINAL PREDICTIONS...")
test_predictions_proba = best_xgb.predict_proba(X_test_final)[:, 1]
test_predictions = (test_predictions_proba >= 0.45).astype(int)

print(f"‚úÖ PREDICTIONS GENERATED:")
print(f"Total test loans: {len(test_predictions)}")
print(f"Predicted Good (0): {(test_predictions == 0).sum()} ({(test_predictions == 0).sum()/len(test_predictions)*100:.1f}%)")
print(f"Predicted Bad (1): {(test_predictions == 1).sum()} ({(test_predictions == 1).sum()/len(test_predictions)*100:.1f}%)")

# Create submission
print("\nüìÑ CREATING FINAL SUBMISSION...")
submission_df = pd.DataFrame({
    'customerID': df_test_final_fixed['customerid'],
    'Good_Bad_flag': test_predictions
})

submission_file = 'loan_default_predictions.csv'
submission_df.to_csv(submission_file, index=False)

print(f"‚úÖ FINAL SUBMISSION FILE: {submission_file}")
print(f"First 10 predictions:")
print(submission_df.head(10).to_string(index=False))

print(f"\nüéâ PROJECT COMPLETE!")
print(f"From raw data ‚Üí trained model ‚Üí test predictions ‚Üí submission file")

# Export current notebook to Python script
from google.colab import files

# Method 1: Using built-in Colab function
!jupyter nbconvert --to script '/content/your_notebook.ipynb'

# Method 2: If you want to download directly
files.download('/content/your_notebook.py')

# Method 3: If the above doesn't work, use this:
!jupyter nbconvert --to python *.ipynb